# test_parsers_chunking.py
"""Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ù„Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ù„ØªÙ‚Ø³ÙŠÙ…"""

import asyncio
from pathlib import Path

# Ø§Ø®ØªØ¨Ø§Ø± PDF Parser
print("=" * 60)
print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± PDF Parser")
print("=" * 60)

from document_processing.parsers.pdf_parser import PDFParser

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù PDF ØªØ¬Ø±ÙŠØ¨ÙŠ (Ù†ØµÙŠ Ø¨Ø³ÙŠØ·)
test_pdf = Path("test_document.txt")
test_pdf.write_text("""ØµÙØ­Ø© 1
Ù‡Ø°Ø§ Ù…Ø³ØªÙ†Ø¯ ØªØ¬Ø±ÙŠØ¨ÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.
ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø© ÙÙ‚Ø±Ø§Øª ÙˆØ¬Ù…Ù„.

ØµÙØ­Ø© 2
This is a test document in English.
It contains multiple paragraphs and sentences.
""", encoding='utf-8')

print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù†ØµÙŠ ØªØ¬Ø±ÙŠØ¨ÙŠ")

# Ø§Ø®ØªØ¨Ø§Ø± Text Splitter
print("\n" + "=" * 60)
print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Text Splitter")
print("=" * 60)

from document_processing.chunking.text_splitter import TextSplitter

splitter = TextSplitter(chunk_size=100, chunk_overlap=20)
print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Text Splitter (size={splitter.chunk_size}, overlap={splitter.chunk_overlap})")

long_text = """
Ù‡Ø°Ø§ Ù†Øµ Ø·ÙˆÙŠÙ„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ‚Ø³ÙŠÙ…. Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ ØªØªØ­Ø¯Ø« Ø¹Ù† Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù‡Ù….
Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© ØªÙƒÙ…Ù„ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙˆØªØ¶ÙŠÙ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©.
Ø§Ù„ÙÙ‚Ø±Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© ØªØ®ØªØªÙ… Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯.

This is a long English text that needs splitting. The first paragraph talks about an important topic.
The second paragraph continues the topic and adds new information.
The third paragraph concludes the topic nicely.
""" * 3  # ØªÙƒØ±Ø§Ø± Ù„Ø¬Ø¹Ù„ Ø§Ù„Ù†Øµ Ø£Ø·ÙˆÙ„

chunks = splitter.split_text(long_text)
print(f"âœ… ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ {len(chunks)} Ù‚Ø·Ø¹Ø©")

for i, chunk in enumerate(chunks[:3], 1):
    print(f"\nØ§Ù„Ù‚Ø·Ø¹Ø© {i} ({len(chunk)} Ø­Ø±Ù):")
    print(chunk[:100] + "..." if len(chunk) > 100 else chunk)

# Ø§Ø®ØªØ¨Ø§Ø± Multilingual Splitter
print("\n" + "=" * 60)
print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Multilingual Splitter (Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)")
print("=" * 60)

from document_processing.chunking.multilingual_splitter import MultilingualTextSplitter

ml_splitter = MultilingualTextSplitter(chunk_size=150, chunk_overlap=30)
print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Multilingual Splitter")

arabic_text = """
Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙŠÙ‡ØªÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù†Ø¸Ù…Ø© Ø°ÙƒÙŠØ©. 
Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…Ø­ÙŠØ·Ø©.
ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆÙ…ØªÙ†ÙˆØ¹Ø© ÙÙŠ Ù…Ø®ØªÙ„Ù Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª.
ÙŠØ³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ø·Ø¨ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØ§Ù„ØµÙ†Ø§Ø¹Ø© ÙˆØ§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ©.
Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ ÙˆØ§Ø¹Ø¯ Ø¬Ø¯Ø§Ù‹ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ù…Ù‡Ù….
"""

# Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù„ØºØ©
stats = ml_splitter.get_language_stats(arabic_text)
print(f"\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù„ØºØ©:")
print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø­Ø±Ù: {stats['total_chars']}")
print(f"   Ø£Ø­Ø±Ù Ø¹Ø±Ø¨ÙŠØ©: {stats['arabic_chars']} ({stats['arabic_percentage']:.1f}%)")
print(f"   Ø£Ø­Ø±Ù Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: {stats['english_chars']} ({stats['english_percentage']:.1f}%)")
print(f"   Ø§Ù„Ù„ØºØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {stats['primary_language']}")

# Ø§Ù„ØªÙ‚Ø³ÙŠÙ…
arabic_chunks = ml_splitter.split_text(arabic_text)
print(f"\nâœ… ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¥Ù„Ù‰ {len(arabic_chunks)} Ù‚Ø·Ø¹Ø©")

for i, chunk in enumerate(arabic_chunks, 1):
    print(f"\nØ§Ù„Ù‚Ø·Ø¹Ø© {i} ({len(chunk)} Ø­Ø±Ù):")
    print(chunk)

# Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØµÙÙŠØ©
print("\n" + "=" * 60)
print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©")
print("=" * 60)

chunks_with_meta = splitter.get_chunks_with_metadata(long_text, "doc_123")
print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(chunks_with_meta)} Ù‚Ø·Ø¹Ø© Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØµÙÙŠØ©")

print(f"\nÙ…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰:")
print(chunks_with_meta[0])

# ØªÙ†Ø¸ÙŠÙ
test_pdf.unlink()

print("\n" + "=" * 60)
print("ğŸ‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù†Ø¬Ø­Øª!")
print("=" * 60)